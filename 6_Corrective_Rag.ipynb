{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement ython-dotenv (from versions: none)\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: No matching distribution found for ython-dotenv\n"
     ]
    }
   ],
   "source": [
    "%pip install -q python-dotenv langchain-openai langgraph langchain_chroma\n",
    "%pip install -qU pypdf langchain-community langchain-text-splitters ython-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 11\n",
      "python-dotenv could not parse statement starting at line 13\n",
      "python-dotenv could not parse statement starting at line 14\n",
      "python-dotenv could not parse statement starting at line 15\n",
      "python-dotenv could not parse statement starting at line 16\n",
      "python-dotenv could not parse statement starting at line 18\n",
      "python-dotenv could not parse statement starting at line 19\n",
      "python-dotenv could not parse statement starting at line 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python-dotenv could not parse statement starting at line 11\n",
      "python-dotenv could not parse statement starting at line 13\n",
      "python-dotenv could not parse statement starting at line 14\n",
      "python-dotenv could not parse statement starting at line 15\n",
      "python-dotenv could not parse statement starting at line 16\n",
      "python-dotenv could not parse statement starting at line 18\n",
      "python-dotenv could not parse statement starting at line 19\n",
      "python-dotenv could not parse statement starting at line 20\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_function = OpenAIEmbeddings(model='text-embedding-3-large')\n",
    "\n",
    "vector_store = Chroma(\n",
    "    embedding_function=embedding_function,\n",
    "    collection_name = 'income_tax_collection',\n",
    "    persist_directory='./income_tax_collection'\n",
    ")\n",
    "retriever = vector_store.as_retriever(search_kwargs={'k': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    query: str\n",
    "    context: str\n",
    "    answer: str\n",
    "    retry_count: int\n",
    "    max_retries: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    사용자의 질문에 기반하여 벡터 스토어에서 관련 문서를 검색합니다.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): 사용자의 질문을 포함한 에이전트의 현재 state.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: 검색된 문서가 추가된 state를 반환합니다.\n",
    "    \"\"\"\n",
    "    query = state['query']\n",
    "    docs = retriever.invoke(query)\n",
    "    return {'context': docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 허브에서 RAG 프롬프트를 가져옵니다\n",
    "generate_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# 지정된 매개변수로 언어 모델을 초기화합니다\n",
    "generate_llm = ChatOpenAI(model='gpt-4o', max_completion_tokens=100)\n",
    "\n",
    "def generate(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    주어진 state를 기반으로 RAG 체인을 사용하여 응답을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): 사용자의 질문과 문맥을 포함한 에이전트의 현재 state.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: 생성된 응답을 포함하는 state를 반환합니다.\n",
    "    \"\"\"\n",
    "    # state에서 컨텍스트와 쿼리를 추출합니다\n",
    "    context = state['context']\n",
    "    query = state['query']\n",
    "    \n",
    "    # 작업 체인을 생성합니다: 먼저 프롬프트를 검색하고, 그 다음 응답을 생성합니다\n",
    "    rag_chain = generate_prompt | generate_llm\n",
    "    \n",
    "    # 쿼리와 컨텍스트로 체인을 호출하여 응답을 얻습니다\n",
    "    response = rag_chain.invoke({'question': query, 'context': context})\n",
    "    \n",
    "    # 생성된 답변을 반환합니다\n",
    "    return {'answer': response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from typing import Literal\n",
    "\n",
    "doc_relevance_prompt = hub.pull(\"langchain-ai/rag-document-relevance\")\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4o', max_completion_tokens=100)\n",
    "\n",
    "def check_doc_relevance(state: AgentState) -> Literal['relevant', 'irrelevant']:\n",
    "    \"\"\"\n",
    "    주어진 state를 기반으로 문서의 관련성을 판단합니다.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): 사용자의 질문과 문맥을 포함한 에이전트의 현재 state.\n",
    "\n",
    "    Returns:\n",
    "        Literal['relevant', 'irrelevant']: 문서가 관련성이 높으면 'relevant', 그렇지 않으면 'irrelevant'를 반환합니다.\n",
    "    \"\"\"\n",
    "    query = state['query']\n",
    "    context = state['context']\n",
    "\n",
    "    doc_relevance_chain = doc_relevance_prompt | llm\n",
    "    response = doc_relevance_chain.invoke({'question': query, 'documents': context})\n",
    "\n",
    "    if response['Score'] == 1:\n",
    "        # 2.3장과 다르게 `relevant`와 `irrelevant`를 반환합니다\n",
    "        # node를 직접 지정하는 것보다 실제 판단 결과를 리턴하면서 해당 node의 재사용성을 높일 수 있습니다.\n",
    "        return 'relevant'\n",
    "    \n",
    "    return 'irrelevant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "rewrite_prompt = PromptTemplate.from_template(f\"\"\"\n",
    "사용자의 질문을 보고, 우리의 웹검색에 용이하게 사용자의 질문을 수정해주세요 \n",
    "                                         \n",
    "질문: {{query}}\n",
    "\"\"\")\n",
    "\n",
    "def rewrite(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    사용자의 질문을 사전을 참고하여 변경합니다.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): 사용자의 질문을 포함한 에이전트의 현재 state.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: 변경된 질문을 포함하는 state를 반환합니다.\n",
    "    \"\"\"\n",
    "\n",
    "    query = state['query']\n",
    "    retry_count = state.get('retry_count', 0)\n",
    "\n",
    "    rewrite_chain = rewrite_prompt | llm | StrOutputParser()\n",
    "\n",
    "    response = rewrite_chain.invoke({'query': query})\n",
    "\n",
    "    return {'query': response, 'retry_count': retry_count + 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bitcamp\\AppData\\Local\\Temp\\ipykernel_17820\\1336542735.py:3: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tavily_search_tool = TavilySearchResults(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import TavilySearchResults\n",
    "\n",
    "tavily_search_tool = TavilySearchResults(\n",
    "    max_results=3,\n",
    "    search_depth=\"advanced\",\n",
    "    include_answer=True,\n",
    "    include_raw_content=True,\n",
    "    include_images=True,\n",
    ")\n",
    "\n",
    "def web_search(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    주어진 state를 기반으로 웹 검색을 수행합니다.\n",
    "\n",
    "    Args:\n",
    "        state (AgentState): 사용자의 질문을 포함한 에이전트의 현재 state.\n",
    "\n",
    "    Returns:\n",
    "        AgentState: 웹 검색 결과가 추가된 state를 반환합니다.\n",
    "    \"\"\"\n",
    "    query = state['query']\n",
    "    results = tavily_search_tool.invoke(query)\n",
    "\n",
    "    return {'context': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단일 conditional edge로 처리\n",
    "def check_doc_relevance_and_route(state: AgentState) -> Literal['relevant', 'rewrite', 'web_search', 'end']:\n",
    "    \n",
    "    query = state['query']\n",
    "    context = state['context']\n",
    "    retry_count = state.get('retry_count', 0)\n",
    "\n",
    "    doc_relevance_chain = doc_relevance_prompt | llm\n",
    "    \n",
    "    response = doc_relevance_chain.invoke({'question': query, 'documents': context})\n",
    "\n",
    "    if response['Score'] == 1:\n",
    "        return 'relevant'\n",
    "\n",
    "    else:\n",
    "        # 관련성이 없을 때 재시도 횟수에 따라 라우팅\n",
    "        if retry_count == 0:\n",
    "            return 'rewrite'\n",
    "        elif retry_count <= 2:  # 1, 2번째 재시도\n",
    "            return 'web_search'\n",
    "        else:\n",
    "            return 'end'  # 3번 이상이면 종료\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1c1292696a0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.add_node('retrieve', retrieve_node)\n",
    "graph_builder.add_node('generate', generate)\n",
    "graph_builder.add_node('rewrite', rewrite)\n",
    "graph_builder.add_node('web_search', web_search)\n",
    "graph_builder.add_node('check_doc_relevance_and_route', check_doc_relevance_and_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
